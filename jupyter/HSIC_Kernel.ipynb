{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSIC bottleneck toy summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pseudo code\n",
    "\n",
    "    train_hsic.py ->  train_misc.py - >   hsic.py       \n",
    "        hsic_train()    hsic_objective()   hsic_normalized_cca()\n",
    "        \n",
    "    xout, [x0,x1,...,xout] = model(data)\n",
    "        \n",
    "    for xi, layer in [x0,x1,...,xout], [lay0, lay1,..., layn]:\n",
    "        # xi, data, target .shape = (m, dims)\n",
    "        hx = hisc(xi, data, σ)\n",
    "        hy = hisc(xi, target, σ)\n",
    "\n",
    "        loss = hx - λhy\n",
    "        sgd_update(layer.weight, loss)\n",
    "        \n",
    "    hsic()\n",
    "        Kx = e^-((X.X - 2XxXT + X.XT^2) /var ) x (1 - I/m)\n",
    "        Rx = K x (ϵ+K^-1)\n",
    "        \n",
    "    hsic = Sum(Rx . RyT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distmat_old(X):\n",
    "    \"\"\" distance matrix\n",
    "    \"\"\"\n",
    "    r = torch.sum(X*X, 1)\n",
    "    r = r.view([-1, 1])\n",
    "    \n",
    "    a = torch.mm(X, torch.transpose(X,0,1))\n",
    "    \n",
    "    D = r.expand_as(a) - 2*a +  torch.transpose(r,0,1).expand_as(a)\n",
    "    D = torch.abs(D)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distmat(x, requires_grad=False):\n",
    "    _cloned = False\n",
    "    if x.requires_grad and not requires_grad:\n",
    "        x = x.clone().detach()\n",
    "        _cloned = True\n",
    "    out = torch.mm(x, x.T).mul_(-2.0)\n",
    "    out.add_((x*x).sum(1, keepdim=True))\n",
    "    out.add_((x*x).sum(1, keepdim=True).T)\n",
    "    if _cloned: \n",
    "        del x\n",
    "    return out.abs_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distmat with ridiculous batch size 256 * 256 \n",
    "    * distmat(d) alocates 16G, distmat_old(d) 32G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data = lambda m, device=\"cpu\": torch.randn(m*m, device=device).view(-1,1)\n",
    "\n",
    "# dlen = 256\n",
    "# data = gen_data(dlen, \"cuda\") # hsic gets data passed as a blockd\n",
    "# print(data.shape)\n",
    "# # torch.Size([65536, 1])\n",
    "\n",
    "# x = distmat(data) # ok\n",
    "#del x\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "# x = distmatold(data) # fail to allocate\n",
    "#del x\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### speed test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gen_data(5, \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit x=distmat2b(d); del x\n",
    "# 21.6 µs ± 98.2 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit x=distmat_old(d); del x\n",
    "# 27.8 µs ± 454 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validate behavior, run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "hsicpath = \"/home/z/work/HSIC-bottleneck/source\"\n",
    "if hsicpath not in sys.path:\n",
    "    sys.path.append(hsicpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hsicbt\n",
    "from hsicbt.model.mhconv import ModelConv\n",
    "from hsicbt.model.mresconv import ModelResConv\n",
    "# from HSIC-bottleneck/config/resconv-hsicbt.yaml\n",
    "sigma = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = ModelResConv(last_hidden_width=512)\n",
    "M.to(device='cuda')\n",
    "t = torch.randn(512,1,28,28, device=\"cuda\") # standard MNIST\n",
    "# for different image size pass: in_width=h*w\n",
    "\n",
    "output, hiddens = M(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dims of latent data [4, 4, 4, 4, 4, 4, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([512, 64, 12, 12]),\n",
       " torch.Size([512, 64, 12, 12]),\n",
       " torch.Size([512, 64, 12, 12]),\n",
       " torch.Size([512, 64, 12, 12]),\n",
       " torch.Size([512, 64, 12, 12]),\n",
       " torch.Size([512, 64, 12, 12]),\n",
       " torch.Size([512, 512])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"dims of latent data\", [len(h.shape) for h in hiddens])\n",
    "[h.shape for h in hiddens]#, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size  (512, 1, 28, 28) \t-> (512, 784)\n",
      "hidden size (512, 64, 12, 12) \t-> (512, 9216)\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "# to run hsic\n",
    "h = hiddens[i].view(-1, np.prod(hiddens[i].size()[1:]))\n",
    "h_data = t.view(-1, np.prod(t.size()[1:]))\n",
    "print(\"input size \", tuple(t.shape),\"\\t->\", tuple(h_data.shape))\n",
    "print(\"hidden size\", tuple(hiddens[i].shape), \"\\t->\", tuple(h.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check values of distmat() vs distmat_old(): ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distmat(h) == distmat_old(h):  True (512, 512)\n"
     ]
    }
   ],
   "source": [
    "do = distmat_old(h)\n",
    "d = distmat(h, requires_grad=False)\n",
    "print(\"distmat(h) == distmat_old(h): \",(d==do).all().item(), tuple(d.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden[i].requires_grad\t\t True \tdevice cuda:0\n",
      "distmat_old(h).requires_grad\t True \tdevice cuda:0\n",
      "distmat(h, False).requires_grad\t False \tdevice cuda:0\n"
     ]
    }
   ],
   "source": [
    "# distmat(x, requires_grad=False) # removes gradient\n",
    "print(\"hidden[i].requires_grad\\t\\t\", h.requires_grad, \"\\tdevice\", h.device)\n",
    "print(\"distmat_old(h).requires_grad\\t\", do.requires_grad, \"\\tdevice\", do.device)\n",
    "print(\"distmat(h, False).requires_grad\\t\",d.requires_grad, \"\\tdevice\", d.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, dim = h.shape\n",
    "variance = (2.* sigma * sigma* dim)\n",
    "k = torch.exp(-d / variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use in place operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9864, 0.9867,  ..., 0.9852, 0.9849, 0.9873],\n",
       "        [0.9864, 1.0000, 0.9863,  ..., 0.9840, 0.9857, 0.9872],\n",
       "        [0.9867, 0.9863, 1.0000,  ..., 0.9855, 0.9868, 0.9868],\n",
       "        ...,\n",
       "        [0.9852, 0.9840, 0.9855,  ..., 1.0000, 0.9853, 0.9861],\n",
       "        [0.9849, 0.9857, 0.9868,  ..., 0.9853, 1.0000, 0.9862],\n",
       "        [0.9873, 0.9872, 0.9868,  ..., 0.9861, 0.9862, 1.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp_(d.mul_(-1.0/variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^(-d/var) in place is equal to old: True\n"
     ]
    }
   ],
   "source": [
    "print(\"e^(-d/var) in place is equal to old:\", (d==k).all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernelmat(X, sigma=None, requires_grad=False):\n",
    "    \"\"\" kernel matrix baker\n",
    "        Args\n",
    "            X             (tensor_ shape (batchsize, datadimension)\n",
    "            sigma         (float [None]) from config\n",
    "            requires_grad (bool [False]) removes gradient from output\n",
    "    \"\"\"\n",
    "    m, dim = X.size()\n",
    "    H = torch.eye(m, device=X.device).sub_(1/m) \n",
    "    Kx = distmat(X, requires_grad=requires_grad)\n",
    "\n",
    "    if sigma:\n",
    "        variance = 2.*sigma*sigma*dim  \n",
    "        torch.exp_(Kx.mul_(-1.0/variance))\n",
    "    else:\n",
    "        try:\n",
    "            sx = sigma_estimation(X, X)\n",
    "            variance = 2.*sx*sx\n",
    "            torch.exp_(Kx.mul_(-1.0/variance))\n",
    "        except RuntimeError as e:\n",
    "            raise RuntimeError(\"Unstable sigma {} with maximum/minimum input ({},{})\".format(\n",
    "                sx, torch.max(X), torch.min(X)))\n",
    "\n",
    "    Kxc =  torch.mm(Kx, H)\n",
    "    del H\n",
    "    del Kx\n",
    "    return Kxc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernelmat_old(X, sigma=None, debug=False, fixdevices=True):\n",
    "    \"\"\" kernel matrix baker\n",
    "    \"\"\"\n",
    "    m = int(X.size()[0])\n",
    "    H = (torch.eye(m) - (1./m) * torch.ones([m, m]))\n",
    "    if fixdevices:\n",
    "        H = H.to(device=X.device)\n",
    "    Dxx = distmat_old(X)\n",
    "\n",
    "    if sigma:\n",
    "        variance = 2.*sigma*sigma*X.size()[1]\n",
    "        if fixdevices:\n",
    "            Kx = torch.exp( -Dxx / variance)\n",
    "        else:\n",
    "            Kx = torch.exp( -Dxx / variance).type(torch.FloatTensor)    # kernel matrices\n",
    "        if debug:\n",
    "            # print(sigma, torch.mean(Kx), torch.max(Kx), torch.min(Kx))\n",
    "            print(\"X    grad\", X.requires_grad, \", device\", X.device)\n",
    "            print(\"Dx   grad\", Dxx.requires_grad, \", device\", Dxx.device)\n",
    "            print(\"Kx   grad\", Kx.requires_grad, \", device\", Kx.device, \"<--.type(torch.FloatTensor) casts a new tensor\") \n",
    "    else:\n",
    "        try:\n",
    "            sx = sigma_estimation(X, X)\n",
    "            if fixdevices:\n",
    "                Kx = torch.exp( -Dxx / (2.*sx*sx))\n",
    "            else:\n",
    "                Kx = torch.exp( -Dxx / (2.*sx*sx)).type(torch.FloatTensor)\n",
    "        except RuntimeError as e:\n",
    "            raise RuntimeError(\"Unstable sigma {} with maximum/minimum input ({},{})\".format(\n",
    "                sx, torch.max(X), torch.min(X)))\n",
    "     \n",
    "    Kxc = torch.mm(Kx, H)\n",
    "\n",
    "    return Kxc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test kernls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X    grad True , device cuda:0\n",
      "Dx   grad True , device cuda:0\n",
      "Kx   grad True , device cuda:0 <--.type(torch.FloatTensor) casts a new tensor\n",
      "Kxc  grad True , device cuda:0\n"
     ]
    }
   ],
   "source": [
    "kxold = kernelmat_old(h, sigma, debug=True)\n",
    "print(\"Kxc  grad\", kxold.requires_grad, \", device\", kxold.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kxc  grad False , device cuda:0\n",
      "Kxc  grad True , device cuda:0\n"
     ]
    }
   ],
   "source": [
    "kx = kernelmat(h, sigma)\n",
    "print(\"Kxc  grad\", kx.requires_grad, \", device\", kx.device)\n",
    "kx = kernelmat(h, sigma, requires_grad=True)\n",
    "print(\"Kxc  grad\", kx.requires_grad, \", device\", kx.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit kxold = kernelmat_old(h, sigma)\n",
    "# # 2.03 ms ± 49.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit kx = kernelmat(h, sigma)\n",
    "# # 817 µs ± 23.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|K_old - K| > 1e-6 False\n"
     ]
    }
   ],
   "source": [
    "e = 1e-6\n",
    "print(\"|K_old - K| > 1e-6\", (torch.abs(kx.clone().detach().cpu()- kxold.clone().detach().cpu()) > e).all().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test hsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_normalized_cca_old(x, y, sigma, use_cuda=True, to_numpy=True, fixdevices=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    m = int(x.size()[0])\n",
    "    Kxc = kernelmat_old(x, sigma=sigma, fixdevices=fixdevices)\n",
    "    Kyc = kernelmat_old(y, sigma=sigma, fixdevices=fixdevices)\n",
    "\n",
    "    epsilon = 1E-5\n",
    "    K_I = torch.eye(m)\n",
    "    if fixdevices:\n",
    "        K_I = K_I.to(device=x.device)\n",
    "    Kxc_i = torch.inverse(Kxc + epsilon*m*K_I)\n",
    "    Kyc_i = torch.inverse(Kyc + epsilon*m*K_I)\n",
    "    Rx = (Kxc.mm(Kxc_i))\n",
    "    Ry = (Kyc.mm(Kyc_i))\n",
    "    Pxy = torch.sum(torch.mul(Rx, Ry.t()))\n",
    "\n",
    "    return Pxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hsic_normalized_cca(x, y, sigma=None, requires_grad=True):\n",
    "    \"\"\" reuse tensors, cleanup, maintains device, cleans grad\n",
    "        x, y of shape (num_batches, -1)\n",
    "    \"\"\"\n",
    "    epsilon = 1E-5\n",
    "    m = x.size()[0]\n",
    "    K_I = torch.eye(m, device=x.device).mul_(epsilon*m)\n",
    "\n",
    "    Kc = kernelmat(x, sigma=sigma, requires_grad=requires_grad)\n",
    "    \n",
    "    Rx = Kc.mm(Kc.add(K_I).inverse())\n",
    "\n",
    "    Kc = kernelmat(y, sigma=sigma, requires_grad=requires_grad)\n",
    "    Ry = Kc.mm(Kc.add(K_I).inverse())\n",
    "\n",
    "    out = Rx.mul_(Ry.t()).sum()\n",
    "    \n",
    "\n",
    "    del Rx\n",
    "    del Ry\n",
    "    del Kc\n",
    "    del K_I\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test new hsic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = ModelResConv(last_hidden_width=512) # for different image size pass: in_width=h*w\n",
    "M.to(device='cuda')\n",
    "t = torch.randn(512,1,28,28, device=\"cuda\") # standard MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hiddens = M(t)\n",
    "\n",
    "i=0\n",
    "# to run hsic\n",
    "h = hiddens[i].view(-1, np.prod(hiddens[i].size()[1:]))\n",
    "h_data = t.view(-1, np.prod(t.size()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(277.2852, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hx = hsic_normalized_cca(h, h_data, sigma, True)\n",
    "hx.backward()\n",
    "print(hx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test old hsic, fixingdevices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hiddens = M(t)\n",
    "\n",
    "i=0\n",
    "h = hiddens[i].view(-1, np.prod(hiddens[i].size()[1:]))\n",
    "h_data = t.view(-1, np.prod(t.size()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(277.2852, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hxo = hsic_normalized_cca_old(h, h_data, sigma, fixdevices=True)\n",
    "#hxo.backward()\n",
    "print(hxo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test old hsic, whitout fixing devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hiddens = M(t)\n",
    "\n",
    "i=0\n",
    "h = hiddens[i].view(-1, np.prod(hiddens[i].size()[1:]))\n",
    "h_data = t.view(-1, np.prod(t.size()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(277.2774, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hxo = hsic_normalized_cca_old(h, h_data, sigma, fixdevices=False)\n",
    "#hxo.backward()\n",
    "print(hxo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
